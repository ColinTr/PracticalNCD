# Software Name : PracticalNCD
# Version: 1.0
# SPDX-FileCopyrightText: Copyright (c) 2023 Orange
# SPDX-License-Identifier: MIT
#
# This software is distributed under the MIT License,
# the text of which is available at https://spdx.org/licenses/MIT.html
# or see the "license.txt" file for more details.

from scipy.optimize import linear_sum_assignment as linear_assignment
import torch.nn.functional as F
import matplotlib.pyplot as plt
import seaborn as sns
import torch.nn as nn
import numpy as np
import logging
import torch
import os


def setup_device(use_cuda=True):
    """
    Initialize the torch device where the code will be executed on.

    :param use_cuda: Set to True if you want the code to be run on your GPU. If set to False, code will run on CPU.
    :return: torch.device : The initialized device, torch.device.
    """
    if use_cuda is False or not torch.cuda.is_available():
        device_name = "cpu"
        if use_cuda is True:
            logging.critical("unable to initialize CUDA, check torch installation (https://pytorch.org/)")
        if use_cuda is False:
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
    else:
        device_name = "cuda:0"
        logging.info("CUDA successfully initialized on device : " + torch.cuda.get_device_name())

    device = torch.device(device_name)

    logging.info("Using device : " + device.type)

    return device


def plot_classes_distribution(y_train, y_unlab, y_test, dataset_name=None):
    fig, axes = plt.subplots(1, 3, figsize=(9, 3))

    train_classes, train_counts = np.unique(y_train, return_counts=True)
    sns.barplot(x=train_classes, y=train_counts, ax=axes[0])
    axes[0].set_title('Train classes distribution')
    axes[0].tick_params(labelrotation=90)

    unlab_classes, unlab_counts = np.unique(y_unlab, return_counts=True)
    sns.barplot(x=unlab_classes, y=unlab_counts, ax=axes[1])
    axes[1].set_title('Unlab classes distribution')
    axes[1].tick_params(labelrotation=90)

    test_classes, test_counts = np.unique(y_test, return_counts=True)
    sns.barplot(x=test_classes, y=test_counts, ax=axes[2])
    axes[2].set_title('Test classes distribution')
    axes[2].tick_params(labelrotation=90)

    if dataset_name is not None:
        plt.suptitle('Distribution of the classes of ' + str(dataset_name))

    plt.tight_layout()


def pairwise_cosine_similarity(input_a, input_b):
    normalized_input_a = torch.nn.functional.normalize(input_a)  
    normalized_input_b = torch.nn.functional.normalize(input_b)
    res = torch.mm(normalized_input_a, normalized_input_b.T)
    return res


def get_activation_function(name):
    if name == 'relu':
        return nn.ReLU()
    elif name == 'sigmoid':
        return nn.Sigmoid()
    elif name == 'tanh':
        return nn.Tanh()
    else:
        return None


def unsupervised_classification_loss(y_pred_1, y_pred_2, labels, eps=1e-7):
    """
    The intuition is that for each pair of samples, if their label is 1, we want the predicted
    probability of the same class to be high, and low if their label is 0.

    :param y_pred_1: The raw class predictions (before softmax) of the first list of samples,
                     torch.tensor of shape (n_samples, n_classes).
    :param y_pred_2: The raw class predictions (before softmax) of the second list of samples,
                     torch.tensor of shape (n_samples, n_classes).
    :param labels: The true labels of the samples (generated by cosine similarity),
                   the value are 0 when the class is different and 1 when it is the same.
                   torch.tensor of shape (n_samples,).
    :param eps: This float is used to 'clip' y_pred_proba's values that are 0, as computing log(0) is impossible.
    :return:
        Mean loss, float.
    """
    prob_1, prob_2 = F.softmax(y_pred_1, -1), F.softmax(y_pred_2, -1)  # Simple softmax of the output
    x = prob_1.mul(prob_2)  # We multiply the prediction of each vector between each other (so same shape is outputted)
    x = x.sum(1)  # We sum the results of each row. If the predictions of the same class were high, the result is close to 1
    return - torch.mean(labels.mul(x.add(eps).log()) + (1 - labels).mul((1 - x).add(eps).log()))  # BCE


def hungarian_accuracy(y_pred, y_true):
    """
    Compute the clustering accuracy.
    The computation is based on the assignment of the most probable clusters using scipy's linear_sum_assignment.

    :param y_pred: ToDo : Documentation
    :param y_true: ToDo : Documentation
    :return: Accuracy between 0 and 1.
    """
    y_true = y_true.astype(np.int64)
    assert y_pred.size == y_true.size
    D = max(y_pred.max(), y_true.max()) + 1
    w = np.zeros((D, D), dtype=np.int64)
    for i in range(y_pred.size):
        w[y_pred[i], y_true[i]] += 1
    ind = linear_assignment(w.max() - w)  # The hungarian algorithm

    acc = sum([w[i, j] for i, j in zip(ind[0], ind[1])]) * 1.0 / y_pred.size

    return acc
